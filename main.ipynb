{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mrMzRsCzwCkk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import pandas\n",
        "from torch.utils.data import Dataset\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RNCyQVN2wCkp"
      },
      "outputs": [],
      "source": [
        "label_mapping = {\n",
        "    0: 0,  # T-shirt/top -> Upper\n",
        "    1: 1,  # Trouser -> Lower\n",
        "    2: 1,  # Pullover -> Lower\n",
        "    3: 0,  # Dress -> Upper\n",
        "    4: 0,  # Coat -> Upper\n",
        "    5: 2,  # Sandal -> Feet\n",
        "    6: 0,  # Shirt -> Upper\n",
        "    7: 2,  # Sneaker -> Feet\n",
        "    8: 3,  # Bag -> Bag\n",
        "    9: 2,  # Ankle boot -> Feet\n",
        "}\n",
        "\n",
        "Word_mapping = {\n",
        "    0: \"Upper\",  # 4000\n",
        "    1: \"Lower\",  # 2000\n",
        "    3: \"Bag\",  # 1000\n",
        "    2: \"Feet\"  # 3000\n",
        "}\n",
        "\n",
        "Num_mapping = {\n",
        "    0: 4000,\n",
        "    1: 2000,\n",
        "    3: 1000,\n",
        "    2: 3000\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u5NXz0o-wCkt"
      },
      "outputs": [],
      "source": [
        "\n",
        "def CalSpace(msg):\n",
        "    return 14 - len(msg)\n",
        "\n",
        "\n",
        "def MakeSpace(msg):\n",
        "    req = CalSpace(msg)\n",
        "    res = \"\"\n",
        "\n",
        "    for _ in range(req):\n",
        "        res = res + \" \"\n",
        "\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R_jgNrq5wCkw"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rQ_grsHtwCkz"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NewTrainDataset(Dataset):\n",
        "    def __init__(self, train_dataframe_feature, train_dataframe_label):\n",
        "        self.dataf = train_dataframe_feature\n",
        "        self.datal = train_dataframe_label\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.dataf[index], label_mapping[self.datal[index]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.datal)\n",
        "\n",
        "\n",
        "class NewTestDataset(Dataset):\n",
        "    def __init__(self, test_dataframe_feature, test_dataframe_label):\n",
        "        self.dataf = test_dataframe_feature\n",
        "        self.datal = test_dataframe_label\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.dataf[index], label_mapping[self.datal[index]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.datal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DycM_92wwCk1"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, hidden1, hidden2, num_out):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3)\n",
        "        self.mxpl1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=4)\n",
        "        self.mxpl2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.linear1 = nn.Linear(\n",
        "            in_features=250, out_features=hidden1, bias=True)\n",
        "        self.linear2 = nn.Linear(\n",
        "            in_features=hidden1, out_features=hidden2, bias=True)\n",
        "        self.linear3 = nn.Linear(\n",
        "            in_features=hidden2, out_features=num_out, bias=False)\n",
        "\n",
        "    def forward(self, data):\n",
        "        out = nn.Tanh()(self.mxpl1(self.conv1(data)))\n",
        "        out = nn.Tanh()(self.mxpl2(self.conv2(out)))\n",
        "\n",
        "        out = out.view(-1, 250)\n",
        "\n",
        "        out = nn.ReLU()(self.linear1(out))\n",
        "        out = nn.ReLU()(self.linear2(out))\n",
        "        out = self.linear3(out)  # RelU not Req\n",
        "\n",
        "        return out\n",
        "\n",
        "# Layer conv => maxpool > activation(TanH) > conv => maxpool => activation(TanH) => Linear => activation (RelU)\n",
        "# => Linear => activation (RelU) => Linear => activation (Not req , Cross Entropy takes care , SoftMax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCBDPSkbwCk5",
        "outputId": "d33ab890-36f3-40d7-a880-d0cd93855836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 18305843.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 337419.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 6038510.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 4694993.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Train Dataset Loaded\n",
            "Test Datset Loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_data = datasets.FashionMNIST(\n",
        "    root='data',\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "print(\"Train Dataset Loaded\")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root='data',\n",
        "    train=False,\n",
        "    download=False,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "print(\"Test Datset Loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcMh103OwCk7",
        "outputId": "7cdaa8fc-a1da-4cfb-bf7d-1234f5e45e7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Feature Shape (60000, 784)\n",
            "Test Dataframe Feature Shape (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "train_dataframe_feature = pandas.DataFrame(\n",
        "    data=(torch.flatten(train_data.data, start_dim=1)).numpy())\n",
        "print(\"Train Dataset Feature Shape {0}\".format(\n",
        "    train_dataframe_feature.shape))\n",
        "\n",
        "test_dataframe_feature = pandas.DataFrame(\n",
        "    data=torch.flatten(input=test_data.data, start_dim=1).numpy())\n",
        "print(\"Test Dataframe Feature Shape {0}\".format(\n",
        "    test_dataframe_feature.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1fezZq2wCk_",
        "outputId": "0c80a466-dd98-4527-dd8e-dbed571728eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataframe Label Shape (60000, 1)\n",
            "Test Dataframe Label Shape (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "train_dataframe_label = pandas.DataFrame(data=train_data.targets.numpy())\n",
        "print(\"Train Dataframe Label Shape {0}\".format(\n",
        "    train_dataframe_label.shape))\n",
        "\n",
        "test_dataframe_label = pandas.DataFrame(data=test_data.targets.numpy())\n",
        "print(\"Test Dataframe Label Shape {0}\".format(test_dataframe_label.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghRV-uMOwClC",
        "outputId": "f391b9a4-6f05-4b7a-9548-fb4ee5843ab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target Labels ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
          ]
        }
      ],
      "source": [
        "print(\"Target Labels {0}\".format(train_data.classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5j65OHlwClF",
        "outputId": "e6ed9e2f-38de-4dbd-85a6-5e5c24d47dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new datasets\n"
          ]
        }
      ],
      "source": [
        "print(\"Creating new datasets\")\n",
        "\n",
        "new_train_dataset = NewTrainDataset(train_dataframe_feature=train_data.data.numpy(\n",
        "), train_dataframe_label=train_data.targets.numpy())\n",
        "new_test_dataset = NewTestDataset(test_dataframe_feature=test_data.data.numpy(\n",
        "), test_dataframe_label=test_data.targets.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "M3_a9vhVwClI"
      },
      "outputs": [],
      "source": [
        "\n",
        "batch_size = 100\n",
        "learning_rate = 1e-2\n",
        "conv_input_size = 125\n",
        "conv_hidden_size_1 = 256\n",
        "conv_hidden_size_2 = 256\n",
        "num_class_out = 4\n",
        "epoches = 5\n",
        "\n",
        "FILE_PATH = \"./model/label10_model.pth\"\n",
        "\n",
        "if not os.path.exists(\"./model\"):\n",
        "    os.mkdir(\"./model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sRJp9liwClK",
        "outputId": "f562cfe6-4f83-44e8-8c15-7bbff4c47cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NewTrain Feature Shape torch.Size([100, 28, 28])\n",
            "NewTrain Label Shape torch.Size([100])\n",
            "NewTest Feature Shape torch.Size([100, 28, 28])\n",
            "NewTest Label Shape torch.Size([100])\n"
          ]
        }
      ],
      "source": [
        "def ViewData(data, prefix=\"\"):\n",
        "    x, y = next(iter(data))\n",
        "    print(\"{1} Feature Shape {0}\".format(x.shape, prefix))\n",
        "    print(\"{1} Label Shape {0}\".format(y.shape, prefix))\n",
        "\n",
        "\n",
        "new_train_dataset_dataloader = DataLoader(\n",
        "dataset=new_train_dataset, num_workers=2, shuffle=True, batch_size=batch_size)\n",
        "new_test_dataset_dataloader = DataLoader(\n",
        "dataset=new_test_dataset, num_workers=2, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "ViewData(new_train_dataset_dataloader, \"NewTrain\")\n",
        "ViewData(new_test_dataset_dataloader, \"NewTest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go-rVv9dwClL",
        "outputId": "00981ef7-49f7-4479-8524-ebeb848cfbe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device used cpu\n"
          ]
        }
      ],
      "source": [
        "print(\"Device used {0}\".format(device.type))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0W1Feq0KwClM"
      },
      "outputs": [],
      "source": [
        "model = ConvNet(hidden1=conv_hidden_size_1,\n",
        "                hidden2=conv_hidden_size_2, num_out=num_class_out)\n",
        "model.to(device=device)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcveyUV7wClO",
        "outputId": "d71c3e6c-dc14-404d-c90a-65c664493484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Model\n",
            "Epoch [1/5] Iter [100/600.0] Loss [0.22570]\n",
            "Epoch [1/5] Iter [200/600.0] Loss [0.16074]\n",
            "Epoch [1/5] Iter [300/600.0] Loss [0.21079]\n",
            "Epoch [1/5] Iter [400/600.0] Loss [0.05851]\n",
            "Epoch [1/5] Iter [500/600.0] Loss [0.12005]\n",
            "Epoch [1/5] Iter [600/600.0] Loss [0.14605]\n",
            "Epoch [2/5] Iter [100/600.0] Loss [0.17093]\n",
            "Epoch [2/5] Iter [200/600.0] Loss [0.14522]\n",
            "Epoch [2/5] Iter [300/600.0] Loss [0.09858]\n",
            "Epoch [2/5] Iter [400/600.0] Loss [0.08663]\n",
            "Epoch [2/5] Iter [500/600.0] Loss [0.20153]\n",
            "Epoch [2/5] Iter [600/600.0] Loss [0.12486]\n",
            "Epoch [3/5] Iter [100/600.0] Loss [0.09861]\n",
            "Epoch [3/5] Iter [200/600.0] Loss [0.14354]\n",
            "Epoch [3/5] Iter [300/600.0] Loss [0.09341]\n",
            "Epoch [3/5] Iter [400/600.0] Loss [0.13002]\n",
            "Epoch [3/5] Iter [500/600.0] Loss [0.15760]\n",
            "Epoch [3/5] Iter [600/600.0] Loss [0.11285]\n",
            "Epoch [4/5] Iter [100/600.0] Loss [0.15624]\n",
            "Epoch [4/5] Iter [200/600.0] Loss [0.09671]\n",
            "Epoch [4/5] Iter [300/600.0] Loss [0.22244]\n",
            "Epoch [4/5] Iter [400/600.0] Loss [0.20442]\n",
            "Epoch [4/5] Iter [500/600.0] Loss [0.15265]\n",
            "Epoch [4/5] Iter [600/600.0] Loss [0.12894]\n",
            "Epoch [5/5] Iter [100/600.0] Loss [0.16071]\n",
            "Epoch [5/5] Iter [200/600.0] Loss [0.14876]\n",
            "Epoch [5/5] Iter [300/600.0] Loss [0.16180]\n",
            "Epoch [5/5] Iter [400/600.0] Loss [0.09872]\n",
            "Epoch [5/5] Iter [500/600.0] Loss [0.11505]\n",
            "Epoch [5/5] Iter [600/600.0] Loss [0.09112]\n",
            "\n",
            "\n",
            "Epoch           1\n",
            "------------------------\n",
            "Average loss      1.05451\n",
            "\n",
            "\n",
            "Epoch           2\n",
            "------------------------\n",
            "Average loss      0.95193\n",
            "\n",
            "\n",
            "Epoch           3\n",
            "------------------------\n",
            "Average loss      0.99155\n",
            "\n",
            "\n",
            "Epoch           4\n",
            "------------------------\n",
            "Average loss      0.92391\n",
            "\n",
            "\n",
            "Epoch           5\n",
            "------------------------\n",
            "Average loss      0.87947\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(FILE_PATH):\n",
        "    print(\"Loading Model\")\n",
        "    model.load_state_dict(torch.load(FILE_PATH))\n",
        "else:\n",
        "    print(\"Saving Model\")\n",
        "    losses = []\n",
        "\n",
        "    model.train(mode=True)\n",
        "    for epoch in range(epoches):\n",
        "        temp =[]\n",
        "        for i, (features, labels) in enumerate(new_train_dataset_dataloader):\n",
        "            features = torch.as_tensor(\n",
        "                data=features, dtype=torch.float32, device=device)\n",
        "            labels = torch.as_tensor(\n",
        "                data=labels, dtype=torch.long, device=device)\n",
        "            features = features.unsqueeze(dim=1)\n",
        "\n",
        "            pred_y = model(features)\n",
        "\n",
        "            loss = loss_func(pred_y, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            temp.append(loss.item())\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(\"Epoch [{0}/{1}] Iter [{2}/{3}] Loss [{4:.5f}]\".format(\n",
        "                    epoch+1, epoches, i+100, 60000/batch_size, loss.item()))\n",
        "        losses.append(temp)\n",
        "\n",
        "    model.train(mode=False)\n",
        "\n",
        "    for index in range(len(losses)):\n",
        "        lossesv = losses[index]\n",
        "        print(\"\\n\\nEpoch           {0}\".format(index+1))\n",
        "        print(\"------------------------\")\n",
        "\n",
        "        avg = 0.0\n",
        "        for value in lossesv:\n",
        "            avg += value / batch_size\n",
        "        \n",
        "        print(\"Average loss      {0:.5f}\".format(avg))\n",
        "\n",
        "\n",
        "    torch.save(model.state_dict(), FILE_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPtqbtXCwClP",
        "outputId": "7121478e-f7e5-4a0a-a452-bbd4c61ac417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Testing Model\n",
            "\n",
            "Overall Accuracy   [0.92980]\n",
            "Name          Accuracy\n",
            "--------------------------\n",
            "Upper         [0.96925]\n",
            "Lower         [0.74600]\n",
            "Feet          [0.99600]\n",
            "Bag           [0.94100]\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\\nTesting Model\")\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_tot_loss = 0.0\n",
        "    correct = 0.0\n",
        "    correctFE = [0.0 for i in range(4)]\n",
        "    boolCrt = []\n",
        "    for i, (features, labels) in enumerate(new_test_dataset_dataloader):\n",
        "        features = torch.as_tensor(\n",
        "            features, dtype=torch.float32, device=device)\n",
        "        labels = torch.as_tensor(\n",
        "            data=labels, dtype=torch.long, device=device)\n",
        "\n",
        "        features = features.squeeze(dim=0)\n",
        "        features = features.unsqueeze(dim=1)\n",
        "\n",
        "        pred_y = model(features)\n",
        "        loss = loss_func(pred_y, labels)\n",
        "        test_tot_loss += (loss.item() / batch_size)\n",
        "\n",
        "        preVal, preValIndexes = torch.max(pred_y, dim=1)\n",
        "\n",
        "        for index in range(len(preVal)):\n",
        "            if torch.eq(preValIndexes[index], labels[index]):\n",
        "                boolCrt.append(preValIndexes[index])\n",
        "\n",
        "    for vi in boolCrt:\n",
        "        correctFE[vi] += 1\n",
        "        correct += 1\n",
        "\n",
        "    print(\"\\nOverall Accuracy   [{0:.5f}]\".format(correct/10000))\n",
        "    print(\"Name          Accuracy\")\n",
        "    print(\"--------------------------\")\n",
        "    for index, value in enumerate(correctFE):\n",
        "        print(\"{0}{1}[{2:.5f}]\".format(Word_mapping[index], MakeSpace(Word_mapping[index]), value/Num_mapping\n",
        "                                        [index]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}